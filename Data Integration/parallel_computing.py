# -*- coding: utf-8 -*-
"""Copy of Parallel Computing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MvBuAE83NOwUHhgqArwuYgXf7iddQd-F
"""

from psutil import *
# This code will return the number of CPU
print("Number of CPU: ", cpu_count())
# This code will return the CPU info
!cat /proc/cpuinfo

cpu_count()

"""# **Counting Words Using Traditional Computing**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import requests
# from bs4 import BeautifulSoup
# import time
# 
# def fetch_and_count_words(url):
#     try:
#         response = requests.get(url)
#         response.raise_for_status()  # Raise an HTTPError for bad responses
#         soup = BeautifulSoup(response.text, 'html.parser')
#         text = soup.get_text()
#         words = text.split()
#         word_count = len(words)
#         return url, word_count
#     except requests.RequestException as e:
#         return url, f"Error: {e}"
# 
# def count_words_in_websites(urls):
#     results = []
#     for url in urls:
#         result = fetch_and_count_words(url)
#         results.append(result)
#     return results
# 
# if __name__ == "__main__":
# 
#     websites = [
#         'https://www.berlin.de/en',
#         'https://en.wikipedia.org/wiki/%22Hello,_World!%22_program',
#         "https://cloud.google.com/learn/what-is-data-integration#:~:text=Get%20the%20report-,Data%20integration%20defined,make%20faster%20and%20better%20decisions.",
#         "https://www.gisma.com/",
#         "https://www.databricks.com/",
#         "https://de.linkedin.com/",
#         "https://spark.apache.org/"
#     ]
# 
#     word_counts = count_words_in_websites(websites)
# 
#     for url, count in word_counts:
# 
#         print(f"URL: {url} - Word Count: {count}")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import time
# import requests
# from bs4 import BeautifulSoup
# from collections import Counter
# 
# def fetch_and_count_words(url):
#     try:
#         response = requests.get(url)
#         response.raise_for_status()  # Raise an HTTPError for bad responses
#         soup = BeautifulSoup(response.text, 'html.parser')
#         text = soup.get_text()
#         words = text.split()
#         word_counts = Counter(words)
#         return url, word_counts
#     except requests.RequestException as e:
#         return url, f"Error: {e}"
# 
# def words_list_in_websites(urls):
#     results = []
#     for url in urls:
#         result = fetch_and_count_words(url)
#         results.append(result)
#     return results
# 
# if __name__ == "__main__":
#     websites = [
#         'https://www.berlin.de/en',
#         'https://en.wikipedia.org/wiki/%22Hello,_World!%22_program',
#         "https://cloud.google.com/learn/what-is-data-integration#:~:text=Get%20the%20report-,Data%20integration%20defined,make%20faster%20and%20better%20decisions.",
#         "https://www.gisma.com/",
#         "https://www.databricks.com/",
#         "https://de.linkedin.com/",
#         "https://spark.apache.org/"
#     ]
# 
#     word_counts = words_list_in_websites(websites)
# 
#     for url, counts in word_counts:
#         if isinstance(counts, Counter):
#             print(f"URL: {url} - Word Counts:")
#             for word, count in counts.items():
#                 print(f"{word}: {count}")
#         else:
#             print(f"URL: {url} - Error: {counts}")
#

"""# **Counting Words Using Parallel Computing**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import requests
# from bs4 import BeautifulSoup
# from concurrent.futures import ProcessPoolExecutor, as_completed
# from time import time
# 
# def fetch_and_count_words(url):
#     try:
#         response = requests.get(url)
#         response.raise_for_status()  # Raise an HTTPError for bad responses
#         soup = BeautifulSoup(response.text, 'html.parser')
#         text = soup.get_text()
#         words = text.split()
#         word_count = len(words)
#         return url, word_count
#     except requests.RequestException as e:
#         return url, f"Error: {e}"
# 
# def count_words_in_websites(urls, num_processes):
#     results = []
#     with ProcessPoolExecutor(max_workers=num_processes) as executor:
#         future_to_url = {executor.submit(fetch_and_count_words, url): url for url in urls}
#         for future in as_completed(future_to_url):
#             url = future_to_url[future]
#             try:
#                 result = future.result()
#             except Exception as exc:
#                 result = (url, f"Error: {exc}")
#             results.append(result)
#     return results
# 
# if __name__ == "__main__":
# 
#     begin = time()
#     websites = [
#         'https://www.berlin.de/en',
#         'https://en.wikipedia.org/wiki/%22Hello,_World!%22_program',
#         "https://cloud.google.com/learn/what-is-data-integration#:~:text=Get%20the%20report-,Data%20integration%20defined,make%20faster%20and%20better%20decisions.",
#         "https://www.gisma.com/",
#         "https://www.databricks.com/",
#         "https://de.linkedin.com/",
#         "https://spark.apache.org/"
#     ]
# 
#     num_processes = 4  # Number of parallel processes
#     word_counts = count_words_in_websites(websites, num_processes)
# 
#     for url, count in word_counts:
#         end = time()
# 
#         print(f"time = {end-begin}")
#         print(f"URL: {url} - Word Count: {count}")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import requests
# from bs4 import BeautifulSoup
# from collections import Counter
# from concurrent.futures import ProcessPoolExecutor, as_completed
# from time import time
# 
# def fetch_and_count_words(url):
#     try:
#         response = requests.get(url)
#         response.raise_for_status()  # Raise an HTTPError for bad responses
#         soup = BeautifulSoup(response.text, 'html.parser')
#         text = soup.get_text()
#         words = text.split()
#         word_counts = Counter(words)
#         return url, word_counts
#     except requests.RequestException as e:
#         return url, f"Error: {e}"
# 
# def count_words_in_websites(urls, num_processes):
#     results = []
#     with ProcessPoolExecutor(max_workers=num_processes) as executor:
#         future_to_url = {executor.submit(fetch_and_count_words, url): url for url in urls}
#         for future in as_completed(future_to_url):
#             url = future_to_url[future]
#             try:
#                 result = future.result()
#             except Exception as exc:
#                 result = (url, f"Error: {exc}")
#             results.append(result)
#     return results
# 
# if __name__ == "__main__":
# 
#     begin = time()
#     websites = [
#         'https://www.berlin.de/en',
#         'https://en.wikipedia.org/wiki/%22Hello,_World!%22_program',
#         "https://cloud.google.com/learn/what-is-data-integration#:~:text=Get%20the%20report-,Data%20integration%20defined,make%20faster%20and%20better%20decisions.",
#         "https://www.gisma.com/faculty-and-team/",
#         "https://www.databricks.com/",
#         "https://de.linkedin.com/",
#         "https://spark.apache.org/"
#     ]
# 
#     num_processes = 4  # Number of parallel processes
#     word_counts = count_words_in_websites(websites, num_processes)
# 
# 
#     for url, counts in word_counts:
#         end = time()
#         print(f"time = {end-begin}")
#         if isinstance(counts, Counter):
#             #print("\n")
#             print("\n       ***************************\n      ")
#             print("\n       ***************************\n      ")
#             print(f"URL: {url} - Word Counts:")
#             for word, count in counts.items():
#                 print(f"{word}: {count}")
#         else:
#             print(f"URL: {url} - Error: {counts}")
# 
# 
# 
#

